<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Play Audio Backwards</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
</head>

<body class="bg-gray-100 flex flex-col gap-4 items-center justify-center h-screen w-screen">
  <div class="gap-1 text-xl grid grid-cols-2 grid-rows-2">
    <button id="startButton" class="bg-blue-500 text-white px-4 py-3 rounded-lg shadow">Start Recording</button>
    <button id="stopButton"  class="bg-red-500 text-white px-4 py-3 rounded-lg shadow">End Recording</button>
    <button id="playButton"  class="bg-green-500 text-white px-4 py-3 rounded-lg shadow">Play audio</button>
    <button id="backwardsButton" class="bg-yellow-500 text-white px-4 py-3 rounded-lg shadow">Play backwards</button>
  </div>
  <!-- <audio id="audio1" controls src=""></audio> -->

  <div class="flex flex-col gap-2 grow">
    <canvas id="waveform" class="bg-white"></canvas>
  </div>
  <!-- audio -->
  <script>
    function get(name) {
      return document.getElementById(name);
    }

    function sleep(ms) {
      return new Promise(resolve => setTimeout(resolve, ms));
    }

    // audioContext is destroyed when recording stops, and recreated when playing and recording again
    let audioContext = null;
    // This needs to be initialized in user event, eg. click, to ask for permission to use microphone
    // We also needs this global var to stop recording
    let mediaStream = null; 
    let sampleChunks = []; // pieces of audio file data appended when recording
    let samples = null; // concatenated AudioBuffer when recording stops
    let reversedSamples = null; // calculated AudioBuffer when recording stops
    
    get('startButton').addEventListener('click', async () => {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const source = audioContext.createMediaStreamSource(mediaStream);
      const processor = audioContext.createScriptProcessor(4096, 2, 1);
      sampleChunks = [];

      // to stop recording: mediaStream.getTracks().forEach(track => track.stop());
      processor.onaudioprocess = (e) => {
        const chunk = e.inputBuffer;
        // we need to copy the data, because the buffer is reused
        const chunkCopy = [new Float32Array(chunk.getChannelData(0)), new Float32Array(chunk.getChannelData(1))];
        sampleChunks.push(chunkCopy);
      }
      const silentNode = audioContext.createGain();
      silentNode.gain.value = 0;

      source.connect(processor);
      // to avoid garbage collection:
      processor.connect(silentNode);
      silentNode.connect(audioContext.destination);
    });

    get('stopButton').addEventListener('click', async () => {
      if (! mediaStream) {
        return
      }
      mediaStream.getTracks().forEach(track => track.stop());
      audioContext.close();

      await sleep(1);

      const length = sampleChunks.reduce((acc, chunk) => acc + chunk[0].length, 0);
      samples = audioContext.createBuffer(2, length, audioContext.sampleRate);
      let offset = 0;
      for (let chunk of sampleChunks) {
        samples.copyToChannel(chunk[0], 0, offset);
        samples.copyToChannel(chunk[1], 1, offset);
        offset += chunk[0].length;
      }

      visualizeAudio(Array.from(samples.getChannelData(0)));

      offset = length;
      const reversedLeft = new Float32Array(length);
      const reversedRight = new Float32Array(length);
      for (let chunk of sampleChunks) {
        for (i in chunk[0]) {
          offset--;
          reversedLeft[offset] = chunk[0][i];
          reversedRight[offset] = chunk[1][i];
        }
      }
      reversedSamples = audioContext.createBuffer(2, length, audioContext.sampleRate);
      reversedSamples.copyToChannel(reversedLeft, 0);
      reversedSamples.copyToChannel(reversedRight, 1);
    });

    get('playButton').addEventListener('click', () => {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const source = audioContext.createBufferSource();
      source.buffer = samples;
      source.connect(audioContext.destination);
      source.start();
    });

    get('backwardsButton').addEventListener('click', () => {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const source = audioContext.createBufferSource();
      source.buffer = reversedSamples;
      source.connect(audioContext.destination);
      source.start();
    });

    function getCanvas(name) {
      const canvas = get(name);
      const context = canvas.getContext('2d');

      let width = window.innerWidth / 5 * 4;
      let height = window.innerHeight - 250;
      width = Math.floor(width);
      height = Math.floor(height);
      console.log(width, height, window.devicePixelRatio);
      // high DPI canvas
      canvas.width = width * window.devicePixelRatio;
      canvas.height = height * window.devicePixelRatio;
      canvas.style.width = `${width}px`;
      canvas.style.height = `${height}px`;
      
      context.strokeStyle = 'black';
      context.beginPath();
      context.moveTo(0, 0);
      context.lineTo(canvas.width, canvas.height);
      context.stroke();

      return [canvas, context];
    }

    getCanvas('waveform')

    function visualizeAudio(data) {
      // data has much more elements than canvas1.width
      // so we need to divide the data into chunks, each with length of step
      const [canvas1, ctx1] = getCanvas('waveform');

      let step = Math.ceil(data.length / canvas1.width);
      if (step == 0) {
        step = 1;
      }
      // we cannot use Math.max(...arr) because we would get stack overflow
      let dataMax = data.map(x => Math.abs(x)).reduce((a, b) => Math.max(a, b));
      // let h = canvas1.height / 2;
      function scale(y) {
        y /= dataMax; // y in [-1, 1]
        y = (1 - y) / 2 
        y *= canvas1.height;
        y = Math.floor(y);
        return y;
      }

      // console.log('Data length:', data.length);
      // console.log('Canvas width:', canvas1.width);
      // console.log('Step:', step);
      // console.log('Max:', max);
      window.ctx1 = ctx1;
      
      ctx1.clearRect(0, 0, canvas1.width, canvas1.height);

      ctx1.fillStyle = 'black';
      for (let x = 0; x * step < data.length; x++) {
        const chunk = data.slice(x * step, (x + 1) * step);
        let min = chunk.reduce((a, b) => Math.min(a, b));
        let max = chunk.reduce((a, b) => Math.max(a, b));
        let ymin = Math.floor(scale(max)) - 1;
        let ymax = Math.floor(scale(min)) + 1;
        ctx1.rect(x, ymin, 1, ymax - ymin);
        ctx1.fill();
      }

    }
  </script>
</body>

</html>